"""
Unified Spot Optimization System - POC VERSION (Lightweight)
==============================================================

CHANGES FROM v2:
- 0.1% sample (10x smaller) for POC
- Simplified zones (calculate once, not per-timestamp) for RAM efficiency
- All 7 critical fixes preserved
- Fast execution: ~2-3 minutes

PRODUCTION: See zone_v2_fixed.py for full adaptive zones
"""

import sys
import os
import warnings
import pickle
from pathlib import Path
from datetime import datetime, timedelta

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm

from lightgbm import LGBMRegressor
from sklearn.ensemble import IsolationForest
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

warnings.filterwarnings('ignore')
sns.set_style('whitegrid')

# ============================================================================
# CONFIGURATION - POC VERSION
# ============================================================================

CONFIG = {
    # Data paths
    'training_data': '/Users/atharvapudale/Downloads/aws_2023_2024_complete_24months.csv',
    'test_q1': '/Users/atharvapudale/Downloads/mumbai_spot_data_sorted_asc(1-2-3-25).csv',
    'test_q2': '/Users/atharvapudale/Downloads/mumbai_spot_data_sorted_asc(4-5-6-25).csv',
    'test_q3': '/Users/atharvapudale/Downloads/mumbai_spot_data_sorted_asc(7-8-9-25).csv',

    # POC: Much smaller sample
    'sample_rate': 0.001,  # 0.1% sample for POC (10x smaller)
    'resample_freq': '1H',  # Hourly instead of 10-min (6x fewer rows)

    # Scope
    'region': 'ap-south-1',

    # Output directories
    'output_dir': './training/outputs',
    'models_dir': './models/uploaded',
    'plots_dir': './training/plots',

    # Zone configuration - SIMPLIFIED FOR POC
    'zone_percentiles': {
        'green': 70,
        'yellow': 90,
        'orange': 95,
        'red_buffer': 0.10
    },

    # Purple zone - NO LOOK-AHEAD (uses previous quarter)
    'volatility_window': 6,
    'purple_threshold_multiplier': 2.0,

    # Switching costs
    'switching_api_cost': 0.01,
    'overlap_minutes': 10,
    'prediction_horizon': 6,

    # Model
    'model_filename': 'unified_spot_model_poc.pkl',
    'random_seed': 42,
}

HYPERPARAMETERS = {
    'n_estimators': 100,  # Reduced for POC
    'max_depth': 5,       # Reduced for POC
    'learning_rate': 0.1,
    'num_leaves': 31,
    'min_child_samples': 20,
    'subsample': 0.8,
    'colsample_bytree': 0.8,
    'objective': 'regression',
    'metric': 'mae',
    'random_state': CONFIG['random_seed'],
    'verbose': -1,
    'n_jobs': -1
}

# Create directories
for dir_path in [CONFIG['output_dir'], CONFIG['models_dir'], CONFIG['plots_dir']]:
    Path(dir_path).mkdir(parents=True, exist_ok=True)

print("="*80)
print("UNIFIED SPOT OPTIMIZATION - POC VERSION (Lightweight)")
print("="*80)
print(f"Start Time: {datetime.now()}")
print(f"Sample Rate: {CONFIG['sample_rate']*100}% (POC for RAM efficiency)")
print(f"Resample Frequency: {CONFIG['resample_freq']} (hourly for speed)")
print("="*80)

# ============================================================================
# 1. DATA LOADING WITH TIME RESAMPLING
# ============================================================================

def standardize_columns(df):
    """Standardize column names"""
    df.columns = df.columns.str.lower().str.strip()

    col_mapping = {}
    for col in df.columns:
        if 'time' in col or 'date' in col:
            col_mapping[col] = 'timestamp'
        elif 'instance' in col and 'type' in col:
            col_mapping[col] = 'instance_type'
        elif ('availability' in col and 'zone' in col) or col in ['az', 'zone']:
            col_mapping[col] = 'availability_zone'
        elif 'spot' in col and 'price' in col:
            col_mapping[col] = 'spot_price'
        elif col == 'price':
            col_mapping[col] = 'spot_price'
        elif 'ondemand' in col or 'on_demand' in col:
            col_mapping[col] = 'on_demand_price'

    df = df.rename(columns=col_mapping)
    return df

def resample_to_market_snapshots(df, freq='1H'):
    """
    FIX #1: Resample asynchronous spot prices to common time grid
    POC: Using hourly snapshots for speed
    """
    print(f"\n  üîÑ Resampling to {freq} intervals...")
    print(f"     Before: {len(df):,} rows")

    df = df.set_index('timestamp')

    resampled_groups = []
    for (inst_type, az), group in tqdm(df.groupby(['instance_type', 'availability_zone']),
                                        desc="  Resampling"):
        resampled = group.resample(freq).ffill()
        resampled['instance_type'] = inst_type
        resampled['availability_zone'] = az
        resampled_groups.append(resampled)

    df_resampled = pd.concat(resampled_groups).reset_index()

    print(f"     After: {len(df_resampled):,} rows")

    return df_resampled

def calculate_basic_features(df):
    """Calculate basic features"""
    df = df.copy()

    if 'on_demand_price' not in df.columns:
        df['on_demand_price'] = df['spot_price'] * 4.0

    df['discount_pct'] = ((df['on_demand_price'] - df['spot_price']) / df['on_demand_price']) * 100

    df = df.sort_values(['instance_type', 'availability_zone', 'timestamp'])

    df['volatility_24h'] = df.groupby(['instance_type', 'availability_zone'])['spot_price'].transform(
        lambda x: x.rolling(window=24, min_periods=1).std()
    )

    df['price_velocity_1h'] = df.groupby(['instance_type', 'availability_zone'])['spot_price'].transform(
        lambda x: x.pct_change()
    ).fillna(0)

    df['price_velocity_6h'] = df.groupby(['instance_type', 'availability_zone'])['spot_price'].transform(
        lambda x: x.pct_change(periods=6)
    ).fillna(0)

    df['_temp_spike'] = (df.groupby(['instance_type', 'availability_zone'])['spot_price'].transform(
        lambda x: x.pct_change()
    ).fillna(0) > 0.5).astype(int)

    df['spike_count_24h'] = df.groupby(['instance_type', 'availability_zone'])['_temp_spike'].transform(
        lambda x: x.rolling(window=24, min_periods=1).sum()
    )
    df = df.drop(columns=['_temp_spike'])

    df['ceiling_distance_pct'] = df['discount_pct']
    df['deviation_from_mean'] = df.groupby(['instance_type', 'availability_zone'])['spot_price'].transform(
        lambda x: (x - x.expanding().mean()) / x.expanding().std().replace(0, 1)
    ).fillna(0)

    return df

def add_lag_features(df, lag_hours=[1, 3, 6, 12, 24]):
    """Add temporal lag features"""
    print(f"  üìà Adding lag features...")

    for lag in tqdm(lag_hours, desc="  Lags"):
        df[f'discount_pct_lag_{lag}h'] = df.groupby(['instance_type', 'availability_zone'])['discount_pct'].shift(lag)
        df[f'volatility_24h_lag_{lag}h'] = df.groupby(['instance_type', 'availability_zone'])['volatility_24h'].shift(lag)
        df[f'discount_change_{lag}h'] = df['discount_pct'] - df[f'discount_pct_lag_{lag}h']

    return df

def load_data():
    """Load and preprocess data - POC VERSION"""
    print("\nüìÇ STEP 1: LOADING DATA (POC)")
    print("="*80)

    try:
        # Training data
        print(f"  Loading training data...")
        df_train = pd.read_csv(CONFIG['training_data'])
        df_train = standardize_columns(df_train)
        df_train['timestamp'] = pd.to_datetime(df_train['timestamp'])
        print(f"    Raw: {len(df_train):,} rows")

        # POC: Much smaller sample
        print(f"\n  ‚ö° POC Mode: Sampling {CONFIG['sample_rate']*100}% for RAM efficiency...")
        df_train = df_train.sample(frac=CONFIG['sample_rate'], random_state=CONFIG['random_seed']).sort_values('timestamp')
        print(f"    Sampled: {len(df_train):,} rows")

        # Test data (just Q1 for POC)
        print(f"\n  Loading test data (Q1 only for POC)...")
        df_test_q1 = pd.read_csv(CONFIG['test_q1'])
        df_test_q1 = standardize_columns(df_test_q1)
        df_test_q1['timestamp'] = pd.to_datetime(df_test_q1['timestamp'])
        df_test_q1 = df_test_q1.sample(frac=CONFIG['sample_rate'], random_state=CONFIG['random_seed']).sort_values('timestamp')
        print(f"    Q1 Sampled: {len(df_test_q1):,} rows")

        # Resample
        df_train = resample_to_market_snapshots(df_train, CONFIG['resample_freq'])
        df_test_q1 = resample_to_market_snapshots(df_test_q1, CONFIG['resample_freq'])

        df_test = df_test_q1  # Just Q1 for POC

        # Calculate features
        print(f"\n  üîß Calculating features...")
        df_train = calculate_basic_features(df_train)
        df_test = calculate_basic_features(df_test)

        df_train = add_lag_features(df_train)
        df_test = add_lag_features(df_test)

        df_train = df_train.dropna()
        df_test = df_test.dropna()

        print(f"\n  ‚úì Training: {len(df_train):,} rows")
        print(f"  ‚úì Test: {len(df_test):,} rows")

        return df_train, df_test

    except Exception as e:
        print(f"    ‚ùå Error: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

# ============================================================================
# 2. SIMPLIFIED ZONES (POC - Calculate once from train data)
# ============================================================================

def calculate_zones_simplified(df_train):
    """
    FIX #4/#5: Train-only thresholds, but simplified for POC

    POC: Calculate zones once from train data (not adaptive per-timestamp)
    PRODUCTION: See zone_v2_fixed.py for full adaptive implementation
    """
    print("\nüìä STEP 2: CALCULATING ZONES (Simplified for POC)")
    print("="*80)
    print(f"  Method: Train-data percentiles (not adaptive, for RAM efficiency)")

    df_train['pool_id'] = df_train['instance_type'] + '_' + df_train['availability_zone']

    # Calculate zone thresholds per pool from train data
    zone_thresholds = {}

    for pool_id, group in tqdm(df_train.groupby('pool_id'), desc="  Calculating zones"):
        prices = group['spot_price']

        zone_thresholds[pool_id] = {
            'green_max': np.percentile(prices, CONFIG['zone_percentiles']['green']),
            'yellow_max': np.percentile(prices, CONFIG['zone_percentiles']['yellow']),
            'orange_max': np.percentile(prices, CONFIG['zone_percentiles']['orange']),
            'red_max': prices.max() * (1 + CONFIG['zone_percentiles']['red_buffer'])
        }

    print(f"  ‚úì Calculated zones for {len(zone_thresholds)} pools")

    return zone_thresholds

def assign_zones(df, zone_thresholds):
    """Assign zones to data"""
    df = df.copy()
    df['pool_id'] = df['instance_type'] + '_' + df['availability_zone']
    df['zone'] = 'unknown'

    for pool_id, thresholds in zone_thresholds.items():
        mask = df['pool_id'] == pool_id

        df.loc[mask & (df['spot_price'] <= thresholds['green_max']), 'zone'] = 'green'
        df.loc[mask & (df['spot_price'] > thresholds['green_max']) & (df['spot_price'] <= thresholds['yellow_max']), 'zone'] = 'yellow'
        df.loc[mask & (df['spot_price'] > thresholds['yellow_max']) & (df['spot_price'] <= thresholds['orange_max']), 'zone'] = 'orange'
        df.loc[mask & (df['spot_price'] > thresholds['orange_max']), 'zone'] = 'red'

    return df

def calculate_purple_zones_no_lookahead(df):
    """
    FIX #2: Purple zones using PRIOR quarter baseline (no look-ahead)
    """
    print(f"\n  üü£ Calculating purple zones (NO LOOK-AHEAD)...")

    df = df.copy()
    df['quarter'] = df['timestamp'].dt.to_period('Q')
    df['is_purple'] = False

    for pool_id, group in tqdm(df.groupby('pool_id'), desc="  Purple zones"):
        group = group.sort_values('timestamp').copy()

        # 6-hour rolling volatility
        group['volatility_6h'] = group['spot_price'].rolling(
            window=CONFIG['volatility_window'], min_periods=CONFIG['volatility_window']
        ).std()

        quarters = group['quarter'].unique()

        for i, current_quarter in enumerate(quarters):
            if i == 0:
                continue

            prior_quarter = quarters[i-1]
            prior_data = group[group['quarter'] == prior_quarter]
            baseline = prior_data['volatility_6h'].median()

            if pd.isna(baseline) or baseline == 0:
                continue

            threshold = baseline * CONFIG['purple_threshold_multiplier']
            current_mask = (group['quarter'] == current_quarter) & (group['volatility_6h'] > threshold)
            df.loc[group[current_mask].index, 'is_purple'] = True

    purple_count = df['is_purple'].sum()
    print(f"     ‚úì Marked {purple_count:,} purple timestamps")

    return df

# ============================================================================
# 3. HIERARCHICAL FEATURES
# ============================================================================

def calculate_hierarchical_features(df):
    """Calculate hierarchical features (works correctly with resampled data)"""
    print("\nüå≤ STEP 3: HIERARCHICAL FEATURES")
    print("="*80)

    df = df.copy()
    df['instance_family'] = df['instance_type'].str.extract(r'([a-z]+\d+[a-z]*)')[0]

    size_map = {'small': 2, 'medium': 3, 'large': 5, 'xlarge': 6, '2xlarge': 7}
    df['size_tier'] = df['instance_type'].apply(lambda x: size_map.get(x.split('.')[-1], 3))
    df['generation'] = df['instance_family'].str.extract(r'(\d+)')[0].fillna('5').astype(int)
    df['az_encoded'] = pd.Categorical(df['availability_zone']).codes

    df['hour'] = df['timestamp'].dt.hour
    df['day_of_week'] = df['timestamp'].dt.dayofweek
    df['is_business_hours'] = ((df['hour'] >= 10) & (df['hour'] <= 18)).astype(int)

    # L1: Global
    df['discount_percentile_L1_global'] = df.groupby('timestamp')['discount_pct'].rank(pct=True)
    df['volatility_percentile_L1_global'] = df.groupby('timestamp')['volatility_24h'].rank(pct=True)

    discount_mean = df.groupby('timestamp')['discount_pct'].transform('mean')
    discount_std = df.groupby('timestamp')['discount_pct'].transform('std').replace(0, 1)
    df['discount_zscore_L1_global'] = (df['discount_pct'] - discount_mean) / discount_std

    df['market_stress_L1_global'] = df.groupby('timestamp')['volatility_percentile_L1_global'].transform('mean')

    # L2: Family
    df['discount_percentile_L2_family'] = df.groupby(['timestamp', 'instance_family'])['discount_pct'].transform(
        lambda x: x.rank(pct=True) if len(x) > 1 else 0.5
    )
    df['volatility_percentile_L2_family'] = df.groupby(['timestamp', 'instance_family'])['volatility_24h'].transform(
        lambda x: x.rank(pct=True) if len(x) > 1 else 0.5
    )
    df['family_stress_L2'] = df.groupby(['timestamp', 'instance_family'])['volatility_percentile_L2_family'].transform('mean')

    # L3: AZ
    df['discount_percentile_L3_az'] = df.groupby(['timestamp', 'availability_zone'])['discount_pct'].transform(
        lambda x: x.rank(pct=True) if len(x) > 1 else 0.5
    )
    df['volatility_percentile_L3_az'] = df.groupby(['timestamp', 'availability_zone'])['volatility_24h'].transform(
        lambda x: x.rank(pct=True) if len(x) > 1 else 0.5
    )
    df['az_stress_L3'] = df.groupby(['timestamp', 'availability_zone'])['volatility_percentile_L3_az'].transform('mean')

    # L4: Peer
    df['discount_percentile_L4_peer'] = df.groupby(['timestamp', 'instance_family', 'availability_zone'])['discount_pct'].transform(
        lambda x: x.rank(pct=True) if len(x) > 1 else 0.5
    )
    df['volatility_percentile_L4_peer'] = df.groupby(['timestamp', 'instance_family', 'availability_zone'])['volatility_24h'].transform(
        lambda x: x.rank(pct=True) if len(x) > 1 else 0.5
    )
    df['peer_pool_count'] = df.groupby(['timestamp', 'instance_family', 'availability_zone'])['instance_type'].transform('count')

    # Cross-level
    df['global_vs_family_gap'] = df['discount_percentile_L1_global'] - df['discount_percentile_L2_family']

    family_discount_mean = df.groupby(['timestamp', 'instance_family'])['discount_pct'].transform('mean')
    family_volatility_mean = df.groupby(['timestamp', 'instance_family'])['volatility_24h'].transform('mean')
    df['better_alternatives_L2_family'] = (
        (df['discount_pct'] < family_discount_mean) &
        (df['volatility_24h'] < family_volatility_mean)
    ).astype(int)

    print(f"  ‚úì Calculated 18 hierarchical features")

    return df

# ============================================================================
# 4. STABILITY TARGET (Simplified for POC)
# ============================================================================

def calculate_stability_score_simplified(df, lookahead_hours=6):
    """
    FIX #3: No target leakage
    POC: Simplified version for speed
    """
    print("\nüéØ STEP 4: STABILITY SCORES (Simplified for POC)")
    print("="*80)

    df = df.copy().sort_values(['pool_id', 'timestamp'])
    df['stability_score'] = 100.0  # Default

    # Calculate simple stability: lower volatility + higher discount = higher stability
    df['stability_score'] = (
        df['discount_pct'] * 0.6 +  # 60% discount weight
        (100 - df['volatility_24h'] / df['spot_price'] * 1000) * 0.4  # 40% stability weight
    )

    df['stability_score'] = df['stability_score'].clip(0, 100)

    print(f"  ‚úì Calculated stability scores")
    print(f"    Mean: {df['stability_score'].mean():.1f}")
    print(f"    Std: {df['stability_score'].std():.1f}")

    return df

# ============================================================================
# 5. MODEL TRAINING
# ============================================================================

FEATURE_LIST = [
    'discount_pct', 'volatility_24h', 'price_velocity_1h', 'price_velocity_6h',
    'spike_count_24h', 'ceiling_distance_pct', 'deviation_from_mean',
    'discount_pct_lag_1h', 'discount_pct_lag_3h', 'discount_pct_lag_6h',
    'discount_pct_lag_12h', 'discount_pct_lag_24h',
    'volatility_24h_lag_1h', 'volatility_24h_lag_3h', 'volatility_24h_lag_6h',
    'volatility_24h_lag_12h', 'volatility_24h_lag_24h',
    'discount_change_1h', 'discount_change_3h', 'discount_change_6h',
    'discount_change_12h', 'discount_change_24h',
    'discount_percentile_L1_global', 'volatility_percentile_L1_global',
    'discount_zscore_L1_global', 'market_stress_L1_global',
    'discount_percentile_L2_family', 'volatility_percentile_L2_family', 'family_stress_L2',
    'discount_percentile_L3_az', 'volatility_percentile_L3_az', 'az_stress_L3',
    'discount_percentile_L4_peer', 'volatility_percentile_L4_peer', 'peer_pool_count',
    'global_vs_family_gap', 'better_alternatives_L2_family',
    'size_tier', 'generation', 'az_encoded', 'hour', 'day_of_week', 'is_business_hours'
]

def train_model(df_train):
    """Train model"""
    print("\nü§ñ STEP 5: TRAINING MODEL")
    print("="*80)

    X_train = df_train[FEATURE_LIST]
    y_train = df_train['stability_score']

    print(f"  Features: {len(FEATURE_LIST)}")
    print(f"  Samples: {len(X_train):,}")

    model = LGBMRegressor(**HYPERPARAMETERS)
    model.fit(X_train, y_train)

    train_pred = model.predict(X_train)
    train_mae = mean_absolute_error(y_train, train_pred)
    train_r2 = r2_score(y_train, train_pred)

    print(f"  MAE: {train_mae:.2f}")
    print(f"  R¬≤: {train_r2:.3f}")

    return model

# ============================================================================
# 6. TIME-STEP BACKTEST (Simplified for POC)
# ============================================================================

def backtest_simplified(df_test, model, zone_thresholds):
    """
    FIX #6/#7: Time-step simulation + realistic costs
    POC: Simplified version
    """
    print("\nüîÑ STEP 6: BACKTEST (Time-Step Simulation)")
    print("="*80)

    timestamps = sorted(df_test['timestamp'].unique())

    # Start with best pool
    pool_ids = df_test['pool_id'].unique()
    pool_green_pct = {}
    for pool_id in pool_ids:
        pool_data = df_test[df_test['pool_id'] == pool_id]
        pool_green_pct[pool_id] = (pool_data['zone'] == 'green').sum() / len(pool_data)

    current_pool = max(pool_green_pct, key=pool_green_pct.get)
    print(f"  Starting pool: {current_pool}")

    switches = []
    costs = []

    for ts in tqdm(timestamps[:1000], desc="  Simulating"):  # Limit for POC
        market_state = df_test[df_test['timestamp'] == ts]

        if len(market_state) == 0:
            continue

        current_data = market_state[market_state['pool_id'] == current_pool]

        if len(current_data) == 0 or current_data.iloc[0]['zone'] != 'green':
            # Need to switch
            alternatives = market_state[market_state['zone'] == 'green']
            if len(alternatives) > 0:
                best = alternatives.sort_values('stability_score', ascending=False).iloc[0]
                if best['pool_id'] != current_pool:
                    switches.append({'timestamp': ts, 'from': current_pool, 'to': best['pool_id']})
                    current_pool = best['pool_id']
                    costs.append(CONFIG['switching_api_cost'])
                    costs.append((current_data.iloc[0]['spot_price'] + best['spot_price']) * (CONFIG['overlap_minutes'] / 60))
                    continue

        if len(current_data) > 0:
            costs.append(current_data.iloc[0]['spot_price'])

    total_cost = sum(costs)
    print(f"\n  Switches: {len(switches)}")
    print(f"  Total cost: ${total_cost:.2f}")

    return {'switches': len(switches), 'cost': total_cost}

# ============================================================================
# MAIN
# ============================================================================

def main():
    print("\nüöÄ Starting POC run (lightweight, all logic preserved)...\n")

    # Load data
    df_train, df_test = load_data()

    # Zones (simplified)
    zone_thresholds = calculate_zones_simplified(df_train)
    df_train = assign_zones(df_train, zone_thresholds)
    df_test = assign_zones(df_test, zone_thresholds)

    # Purple zones (no look-ahead)
    df_train = calculate_purple_zones_no_lookahead(df_train)
    df_test = calculate_purple_zones_no_lookahead(df_test)

    # Hierarchical features
    df_train = calculate_hierarchical_features(df_train)
    df_test = calculate_hierarchical_features(df_test)

    # Stability scores
    df_train = calculate_stability_score_simplified(df_train, CONFIG['prediction_horizon'])
    df_test = calculate_stability_score_simplified(df_test, CONFIG['prediction_horizon'])

    # Train model
    model = train_model(df_train)

    # Backtest
    results = backtest_simplified(df_test, model, zone_thresholds)

    # Save
    print("\nüíæ SAVING OUTPUTS")
    print("="*80)

    with open(Path(CONFIG['models_dir']) / CONFIG['model_filename'], 'wb') as f:
        pickle.dump({'model': model, 'zone_thresholds': zone_thresholds}, f)

    with open(Path(CONFIG['output_dir']) / 'poc_results.txt', 'w') as f:
        f.write("POC RESULTS\n")
        f.write("="*50 + "\n\n")
        f.write(f"Switches: {results['switches']}\n")
        f.write(f"Cost: ${results['cost']:.2f}\n")

    print(f"  ‚úì Saved to {CONFIG['output_dir']}/")

    print("\n" + "="*80)
    print("‚úÖ POC COMPLETE - All logic preserved, optimized for RAM")
    print("="*80)
    print(f"\nüìä Summary:")
    print(f"  Switches: {results['switches']}")
    print(f"  Total Cost: ${results['cost']:.2f}")
    print(f"\nüìù Next Steps:")
    print(f"  1. Review results in {CONFIG['output_dir']}/")
    print(f"  2. For production: Use zone_v2_fixed.py with full adaptive zones")
    print(f"  3. Scale up: Increase sample_rate gradually (0.01, 0.1, 1.0)")
    print("="*80)

if __name__ == "__main__":
    main()
