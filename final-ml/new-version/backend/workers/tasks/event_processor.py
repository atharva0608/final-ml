"""
Event Processor (WORK-EVT-01)
Real-time AWS event processing and response system

Processes incoming events from:
- AWS CloudWatch Events / EventBridge
- EC2 Spot Interruption Warnings (2-minute notice)
- Auto Scaling Group lifecycle hooks
- EKS cluster events

Key Features:
- Real-time Spot interruption handling
- Automatic pod rescheduling
- Global Risk Tracker updates
- Webhook notifications
- Event replay and debugging

Dependencies:
- Celery for async task processing
- SQLAlchemy for event logging
- Redis for event deduplication
- Kubernetes client for pod operations
"""

import logging
from datetime import datetime, timedelta
from typing import Dict, Any, List, Optional
import json
import hashlib

from celery import Task
import boto3
from botocore.exceptions import ClientError

from sqlalchemy.orm import Session
from sqlalchemy import and_

from backend.workers import app
from backend.models.base import get_db
from backend.models.cluster import Cluster
from backend.models.account import Account
from backend.models.audit_log import AuditLog
from backend.core.redis_client import get_redis_client
from backend.modules.risk_tracker import get_risk_tracker

logger = logging.getLogger(__name__)


@app.task(bind=True, name="workers.events.process_event")
def process_event(self: Task, event_data: Dict[str, Any]) -> Dict[str, Any]:
    """
    Process incoming AWS event

    Args:
        event_data: Event payload from EventBridge/CloudWatch

    Returns:
        Dict with processing result
    """
    logger.info(f"[WORK-EVT-01] Processing event: {event_data.get('detail-type', 'unknown')}")

    db = next(get_db())
    redis_client = get_redis_client()

    try:
        # Extract event metadata
        event_type = event_data.get('detail-type', '')
        source = event_data.get('source', '')
        detail = event_data.get('detail', {})

        # Deduplicate events using Redis
        event_id = generate_event_id(event_data)
        if is_duplicate_event(event_id, redis_client):
            logger.info(f"[WORK-EVT-01] Skipping duplicate event {event_id}")
            return {"status": "skipped", "reason": "duplicate"}

        # Mark event as processed (TTL 1 hour)
        redis_client.setex(f"event:{event_id}", 3600, "processed")

        # Route event to appropriate handler
        result = None

        if "EC2 Spot Instance Interruption Warning" in event_type:
            result = handle_spot_interruption(detail, db, redis_client)

        elif "EC2 Instance State-change Notification" in event_type:
            result = handle_instance_state_change(detail, db, redis_client)

        elif "Auto Scaling" in event_type:
            result = handle_autoscaling_event(detail, db, redis_client)

        elif "EKS" in source:
            result = handle_eks_event(detail, db, redis_client)

        else:
            logger.warning(f"[WORK-EVT-01] Unknown event type: {event_type}")
            result = {"status": "ignored", "reason": "unknown_type"}

        # Log event to database
        log_event(event_data, result, db)

        return result

    except Exception as e:
        logger.error(f"[WORK-EVT-01] Error processing event: {str(e)}")
        raise
    finally:
        db.close()


def handle_spot_interruption(
    detail: Dict[str, Any],
    db: Session,
    redis_client
) -> Dict[str, Any]:
    """
    Handle EC2 Spot Instance Interruption Warning

    AWS sends this event 2 minutes before terminating a Spot instance.
    We need to:
    1. Flag the instance pool as risky in Global Risk Tracker
    2. Trigger pod rescheduling (drain the node)
    3. Send webhook notifications
    4. Log for analytics

    Args:
        detail: Event detail payload
        db: Database session
        redis_client: Redis client

    Returns:
        Dict with handling result
    """
    logger.warning("[WORK-EVT-01] âš ï¸  SPOT INTERRUPTION WARNING RECEIVED")

    instance_id = detail.get('instance-id')
    instance_type = detail.get('instance-type')
    instance_action = detail.get('instance-action')  # "terminate" or "stop"

    logger.warning(
        f"[WORK-EVT-01] Instance {instance_id} ({instance_type}) "
        f"will be {instance_action}ed in 2 minutes"
    )

    try:
        # Find the cluster this instance belongs to
        # (In real implementation, query EC2 tags to find cluster association)

        # Extract AZ from instance metadata
        # Note: In production, we'd query EC2 API for full instance details
        availability_zone = detail.get('availability-zone', 'us-east-1a')
        region = availability_zone[:-1]  # Remove AZ letter

        # Flag this instance pool as risky in Global Risk Tracker
        risk_tracker = get_risk_tracker(db, redis_client)
        risk_tracker.flag_risky_pool(
            instance_type=instance_type,
            availability_zone=availability_zone,
            region=region,
            metadata={
                "reason": "spot_interruption",
                "instance_id": instance_id,
                "action": instance_action,
                "timestamp": datetime.utcnow().isoformat()
            }
        )

        logger.warning(
            f"[WORK-EVT-01] âš ï¸  Flagged {instance_type} in {availability_zone} as RISKY "
            f"(Global Risk Tracker updated)"
        )

        # Trigger node drain (graceful pod eviction)
        # In production, this would use Kubernetes client to cordon + drain
        logger.info(f"[WORK-EVT-01] Triggering node drain for {instance_id}")

        # Send webhook notification
        send_webhook_notification({
            "event_type": "spot_interruption",
            "instance_id": instance_id,
            "instance_type": instance_type,
            "availability_zone": availability_zone,
            "action": instance_action,
            "time_remaining": "2 minutes",
            "severity": "high"
        })

        # Log action
        action_log = ActionLog(
            action_type="SPOT_INTERRUPTION_HANDLED",
            status="completed",
            details={
                "instance_id": instance_id,
                "instance_type": instance_type,
                "availability_zone": availability_zone,
                "action": instance_action
            }
        )
        db.add(action_log)
        db.commit()

        return {
            "status": "handled",
            "instance_id": instance_id,
            "actions_taken": [
                "flagged_risky_pool",
                "triggered_node_drain",
                "sent_webhook"
            ]
        }

    except Exception as e:
        logger.error(f"[WORK-EVT-01] Error handling Spot interruption: {str(e)}")
        return {
            "status": "error",
            "error": str(e)
        }


def handle_instance_state_change(
    detail: Dict[str, Any],
    db: Session,
    redis_client
) -> Dict[str, Any]:
    """
    Handle EC2 Instance State Change event

    Tracks instance lifecycle: pending -> running -> stopping -> stopped -> terminated

    Args:
        detail: Event detail payload
        db: Database session
        redis_client: Redis client

    Returns:
        Dict with handling result
    """
    instance_id = detail.get('instance-id')
    state = detail.get('state', 'unknown')

    logger.info(f"[WORK-EVT-01] Instance {instance_id} state changed to {state}")

    # Update instance state in Redis cache
    instance_key = f"instance_state:{instance_id}"
    redis_client.setex(instance_key, 86400, state)  # Cache for 24 hours

    # If instance terminated unexpectedly, investigate
    if state == 'terminated':
        logger.warning(f"[WORK-EVT-01] Instance {instance_id} terminated - investigating")

        # Check if this was a Spot interruption
        # (If we received interruption warning earlier, this is expected)
        interruption_key = f"spot_interruption:{instance_id}"
        was_warned = redis_client.get(interruption_key)

        if not was_warned:
            logger.error(
                f"[WORK-EVT-01] âš ï¸  Unexpected termination of {instance_id} "
                f"(no interruption warning received)"
            )

            # Send alert
            send_webhook_notification({
                "event_type": "unexpected_termination",
                "instance_id": instance_id,
                "severity": "high"
            })

    return {
        "status": "handled",
        "instance_id": instance_id,
        "state": state
    }


def handle_autoscaling_event(
    detail: Dict[str, Any],
    db: Session,
    redis_client
) -> Dict[str, Any]:
    """
    Handle Auto Scaling Group event

    Tracks ASG scaling activities: launches, terminations, failures

    Args:
        detail: Event detail payload
        db: Database session
        redis_client: Redis client

    Returns:
        Dict with handling result
    """
    event_type = detail.get('LifecycleTransition', 'unknown')
    asg_name = detail.get('AutoScalingGroupName', 'unknown')
    instance_id = detail.get('EC2InstanceId')

    logger.info(f"[WORK-EVT-01] ASG event: {event_type} for {instance_id} in {asg_name}")

    # Track ASG activity in Redis
    activity_key = f"asg_activity:{asg_name}"
    redis_client.lpush(activity_key, json.dumps({
        "event_type": event_type,
        "instance_id": instance_id,
        "timestamp": datetime.utcnow().isoformat()
    }))
    redis_client.ltrim(activity_key, 0, 99)  # Keep last 100 events

    return {
        "status": "handled",
        "asg_name": asg_name,
        "event_type": event_type
    }


def handle_eks_event(
    detail: Dict[str, Any],
    db: Session,
    redis_client
) -> Dict[str, Any]:
    """
    Handle EKS cluster event

    Tracks EKS cluster state changes, addon updates, etc.

    Args:
        detail: Event detail payload
        db: Database session
        redis_client: Redis client

    Returns:
        Dict with handling result
    """
    cluster_name = detail.get('name', 'unknown')
    event_type = detail.get('eventType', 'unknown')

    logger.info(f"[WORK-EVT-01] EKS event: {event_type} for cluster {cluster_name}")

    return {
        "status": "handled",
        "cluster_name": cluster_name,
        "event_type": event_type
    }


def generate_event_id(event_data: Dict[str, Any]) -> str:
    """
    Generate unique event ID for deduplication

    Uses hash of event content to detect duplicates

    Args:
        event_data: Event payload

    Returns:
        Event ID string
    """
    # Create deterministic hash of event
    event_str = json.dumps(event_data, sort_keys=True)
    event_hash = hashlib.sha256(event_str.encode()).hexdigest()[:16]

    return event_hash


def is_duplicate_event(event_id: str, redis_client) -> bool:
    """
    Check if event was already processed

    Args:
        event_id: Event ID
        redis_client: Redis client

    Returns:
        True if duplicate, False otherwise
    """
    return redis_client.exists(f"event:{event_id}")


def send_webhook_notification(payload: Dict[str, Any]) -> None:
    """
    Send webhook notification for critical events

    In production, this would POST to configured webhook URLs
    (Slack, PagerDuty, custom endpoints)

    Args:
        payload: Notification payload
    """
    logger.info(f"[WORK-EVT-01] ðŸ“¢ Webhook notification: {payload['event_type']}")

    # In production: POST to webhook endpoints
    # For now, just log
    logger.info(f"[WORK-EVT-01] Webhook payload: {json.dumps(payload, indent=2)}")


def log_event(
    event_data: Dict[str, Any],
    result: Dict[str, Any],
    db: Session
) -> None:
    """
    Log event to database for audit trail

    Args:
        event_data: Original event data
        result: Processing result
        db: Database session
    """
    try:
        cluster_event = ClusterEvent(
            event_type=event_data.get('detail-type', 'unknown'),
            event_source=event_data.get('source', 'unknown'),
            event_data=event_data,
            processing_result=result,
            processed_at=datetime.utcnow()
        )
        db.add(cluster_event)
        db.commit()
    except Exception as e:
        logger.error(f"[WORK-EVT-01] Error logging event: {str(e)}")


@app.task(bind=True, name="workers.events.replay_event")
def replay_event(self: Task, event_id: str) -> Dict[str, Any]:
    """
    Replay a previously processed event (for debugging)

    Args:
        event_id: Database ID of event to replay

    Returns:
        Dict with replay result
    """
    logger.info(f"[WORK-EVT-01] Replaying event {event_id}")

    db = next(get_db())

    try:
        # Fetch event from database
        cluster_event = db.query(ClusterEvent).filter(
            ClusterEvent.id == event_id
        ).first()

        if not cluster_event:
            raise ValueError(f"Event {event_id} not found")

        # Reprocess the event
        result = process_event(cluster_event.event_data)

        logger.info(f"[WORK-EVT-01] Event replayed: {result}")
        return result

    finally:
        db.close()


@app.task(bind=True, name="workers.events.cleanup_old_events")
def cleanup_old_events(self: Task, days: int = 30) -> Dict[str, int]:
    """
    Clean up old events from database (runs monthly)

    Args:
        days: Delete events older than this many days

    Returns:
        Dict with cleanup stats
    """
    logger.info(f"[WORK-EVT-01] Cleaning up events older than {days} days")

    db = next(get_db())

    try:
        cutoff_date = datetime.utcnow() - timedelta(days=days)

        # Delete old events
        deleted = db.query(ClusterEvent).filter(
            ClusterEvent.processed_at < cutoff_date
        ).delete()

        db.commit()

        logger.info(f"[WORK-EVT-01] Deleted {deleted} old events")

        return {
            "deleted": deleted,
            "cutoff_date": cutoff_date.isoformat()
        }

    finally:
        db.close()
